{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81828df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "646/646 [==============================] - 14s 18ms/step - loss: 6.3370\n",
      "Epoch 2/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 5.7879\n",
      "Epoch 3/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 5.3413\n",
      "Epoch 4/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 4.9780\n",
      "Epoch 5/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 4.6638\n",
      "Epoch 6/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 4.3907\n",
      "Epoch 7/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 4.1381\n",
      "Epoch 8/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 3.8999\n",
      "Epoch 9/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 3.6785\n",
      "Epoch 10/10\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 3.4626\n",
      "You: hi\n",
      "Bot: i think you can you can you can you can you can you can you can you can\n",
      "You: hello\n",
      "Bot: a good luck for you can you can you can you can you can you can you can\n",
      "You: hello k7\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\1497058630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mbot_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Bot: {bot_response}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\1497058630.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Correct spelling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \"\"\"\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load your dialog dataset\n",
    "# Replace 'ab.csv' with the actual path to your dataset\n",
    "dataset = pd.read_csv('ab.csv', encoding='latin1')\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['a'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize and pad the sequences\n",
    "input_sequences = []\n",
    "for line in dataset['a']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Separate input and target sequences\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# Create and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model with a specified number of epochs\n",
    "model.fit(X, y, epochs=10)  # You can specify the number of epochs here\n",
    "\n",
    "# Save the trained model to a pkl file\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Initialize a spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load the trained model from the pkl file\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Recreate the tokenizer from the training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['a'])\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    user_input = spell.correction(user_input)  # Correct spelling\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=max_sequence_length - 1, padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(max_sequence_length - 1):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64db6e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "646/646 [==============================] - 14s 19ms/step - loss: 6.3331\n",
      "Epoch 2/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 5.8055\n",
      "Epoch 3/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 5.3794\n",
      "Epoch 4/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 5.0141\n",
      "Epoch 5/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 4.6792\n",
      "Epoch 6/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 4.3891\n",
      "Epoch 7/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 4.1261\n",
      "Epoch 8/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 3.8843\n",
      "Epoch 9/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 3.6573\n",
      "Epoch 10/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 3.4473\n",
      "Epoch 11/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 3.2453\n",
      "Epoch 12/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 3.0567\n",
      "Epoch 13/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 2.8795\n",
      "Epoch 14/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 2.7162\n",
      "Epoch 15/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 2.5644\n",
      "Epoch 16/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 2.4272\n",
      "Epoch 17/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 2.2999\n",
      "Epoch 18/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 2.1819\n",
      "Epoch 19/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 2.0721\n",
      "Epoch 20/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.9701\n",
      "Epoch 21/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.8757\n",
      "Epoch 22/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.7901\n",
      "Epoch 23/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.7093\n",
      "Epoch 24/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.6347\n",
      "Epoch 25/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.5664\n",
      "Epoch 26/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.5025\n",
      "Epoch 27/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.4436\n",
      "Epoch 28/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.3906\n",
      "Epoch 29/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.3420\n",
      "Epoch 30/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.2968\n",
      "Epoch 31/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.2554\n",
      "Epoch 32/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.2191\n",
      "Epoch 33/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.1830\n",
      "Epoch 34/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.1502\n",
      "Epoch 35/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 1.1205\n",
      "Epoch 36/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.0961\n",
      "Epoch 37/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.0714\n",
      "Epoch 38/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.0504\n",
      "Epoch 39/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.0308\n",
      "Epoch 40/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 1.0133\n",
      "Epoch 41/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9952\n",
      "Epoch 42/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9795\n",
      "Epoch 43/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9668\n",
      "Epoch 44/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9549\n",
      "Epoch 45/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9417\n",
      "Epoch 46/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9325\n",
      "Epoch 47/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9234\n",
      "Epoch 48/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9134\n",
      "Epoch 49/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.9050\n",
      "Epoch 50/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8981\n",
      "Epoch 51/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8908\n",
      "Epoch 52/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8865\n",
      "Epoch 53/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8805\n",
      "Epoch 54/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 0.8737\n",
      "Epoch 55/100\n",
      "646/646 [==============================] - 13s 20ms/step - loss: 0.8694\n",
      "Epoch 56/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 0.8641\n",
      "Epoch 57/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 0.8612\n",
      "Epoch 58/100\n",
      "646/646 [==============================] - 12s 18ms/step - loss: 0.8569\n",
      "Epoch 59/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8536\n",
      "Epoch 60/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8479\n",
      "Epoch 61/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8462\n",
      "Epoch 62/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8434\n",
      "Epoch 63/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8391\n",
      "Epoch 64/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8366\n",
      "Epoch 65/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8343\n",
      "Epoch 66/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8333\n",
      "Epoch 67/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8294\n",
      "Epoch 68/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8267\n",
      "Epoch 69/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8280\n",
      "Epoch 70/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8262\n",
      "Epoch 71/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8219\n",
      "Epoch 72/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8182\n",
      "Epoch 73/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8182\n",
      "Epoch 74/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8175\n",
      "Epoch 75/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8165\n",
      "Epoch 76/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8148\n",
      "Epoch 77/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8124\n",
      "Epoch 78/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8117\n",
      "Epoch 79/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8107\n",
      "Epoch 80/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8093\n",
      "Epoch 81/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8082\n",
      "Epoch 82/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8064\n",
      "Epoch 83/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8040\n",
      "Epoch 84/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8050\n",
      "Epoch 85/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8032\n",
      "Epoch 86/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8031\n",
      "Epoch 87/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8013\n",
      "Epoch 88/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8004\n",
      "Epoch 89/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.8002\n",
      "Epoch 90/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7993\n",
      "Epoch 91/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7983\n",
      "Epoch 92/100\n",
      "646/646 [==============================] - 13s 20ms/step - loss: 0.7973\n",
      "Epoch 93/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7969\n",
      "Epoch 94/100\n",
      "646/646 [==============================] - 13s 19ms/step - loss: 0.7942\n",
      "Epoch 95/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7952\n",
      "Epoch 96/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7936\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7937\n",
      "Epoch 98/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7943\n",
      "Epoch 99/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7938\n",
      "Epoch 100/100\n",
      "646/646 [==============================] - 12s 19ms/step - loss: 0.7922\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load your dialog dataset\n",
    "# Replace 'ab.csv' with the actual path to your dataset\n",
    "dataset = pd.read_csv('ab.csv', encoding='latin1')\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['a'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize and pad the sequences\n",
    "input_sequences = []\n",
    "for line in dataset['a']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Separate input and target sequences\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# Create and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model with a specified number of epochs\n",
    "model.fit(X, y, epochs=100)  # You can specify the number of epochs here\n",
    "\n",
    "# Save the trained model to a pkl file\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Save the tokenizer to a pkl file\n",
    "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "# Initialize a spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Load the trained model from the pkl file\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Load the tokenizer from the pkl file\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    user_input = spell.correction(user_input)  # Correct spelling\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=max_sequence_length - 1, padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(max_sequence_length - 1):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main\":\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51fe58e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "Bot: there are you don't you don't you don't you don't you don't you don't you don't you don't\n",
      "You: hello\n",
      "Bot: we can we can we can we can we can we can we can we can we can\n",
      "You: hi, how are you doing?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\105595741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mbot_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Bot: {bot_response}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\105595741.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Correct spelling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \"\"\"\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the trained model from the pkl file\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Load the tokenizer from the pkl file\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "# Initialize a spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    user_input = spell.correction(user_input)  # Correct spelling\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=model.input_shape[1], padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(model.input_shape[1]):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e789f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: k7\n",
      "Bot: there are you don't you don't you don't you don't you don't you don't you don't you don't\n",
      "You: vimal\n",
      "Bot: there are you don't you don't you don't you don't you don't you don't you don't you don't\n",
      "You: sairam\n",
      "Bot: there are you don't you don't you don't you don't you don't you don't you don't you don't\n",
      "You: gokul\n",
      "Bot: there are you don't you don't you don't you don't you don't you don't you don't you don't\n",
      "You: vel\n",
      "Bot: there are you don't you don't you don't you don't you don't you don't you don't you don't\n",
      "You: howareyou\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\3203959319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mbot_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbot_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\3203959319.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Correct spelling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \"\"\"\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the trained model from the pkl file\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Load the tokenizer from the pkl file\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "# Initialize a spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Check if user input is not empty\n",
    "    if not user_input:\n",
    "        return \"Bot: Please enter a message.\"\n",
    "\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    user_input = spell.correction(user_input)  # Correct spelling\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=model.input_shape[1], padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(model.input_shape[1]):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return \"Bot: \" + response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(bot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1840f0f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    # Check if user input is not empty\n",
    "    if not user_input:\n",
    "        return \"Bot: Please enter a message.\"\n",
    "\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    corrected_input = spell.correction(user_input)  # Correct spelling\n",
    "\n",
    "    # Check if the correction result is not None\n",
    "    if corrected_input is None:\n",
    "        return \"Bot: I'm sorry, I couldn't understand your input.\"\n",
    "\n",
    "    input_sequence = tokenizer.texts_to_sequences([corrected_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=model.input_shape[1], padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(model.input_shape[1]):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return \"Bot: \" + response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! How can I assist you today?\n",
      "You: hii\n",
      "Bot: doctor you should take me too bad thanks i don't you should take me too bad thanks i don't you should take me too bad thanks i don't you should take\n",
      "You: bye\n",
      "Bot: even though i don't you should take me too bad thanks i don't you should take me too bad thanks i don't you should take me too bad thanks i don't\n",
      "You: goodnight\n",
      "Bot: what do you should take me too bad thanks i don't you should take me too bad thanks i don't you should take me too bad thanks i don't you should\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the trained model from the pkl file\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Load the tokenizer from the pkl file\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "# Initialize a spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Check if user input is not empty\n",
    "    if not user_input:\n",
    "        return \"Bot: Please enter a message.\"\n",
    "\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    corrected_input = spell.correction(user_input)  # Correct spelling\n",
    "\n",
    "    # Check if the correction result is not None\n",
    "    if corrected_input is None:\n",
    "        return \"Bot: I'm sorry, I couldn't understand your input.\"\n",
    "\n",
    "    input_sequence = tokenizer.texts_to_sequences([corrected_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=model.input_shape[1], padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(model.input_shape[1]):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return \"Bot: \" + response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Bot: Hello! How can I assist you today?\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(bot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "730c94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1387/1387 [==============================] - 40s 27ms/step - loss: 6.0202\n",
      "Epoch 2/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 5.1943\n",
      "Epoch 3/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 4.6459\n",
      "Epoch 4/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 4.2044\n",
      "Epoch 5/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 3.8243\n",
      "Epoch 6/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 3.4820\n",
      "Epoch 7/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 3.1748\n",
      "Epoch 8/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 2.8982\n",
      "Epoch 9/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 2.6575\n",
      "Epoch 10/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 2.4427\n",
      "Epoch 11/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 2.2507\n",
      "Epoch 12/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 2.0815\n",
      "Epoch 13/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 1.9255\n",
      "Epoch 14/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 1.7877\n",
      "Epoch 15/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 1.6636\n",
      "Epoch 16/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 1.5486\n",
      "Epoch 17/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.4474\n",
      "Epoch 18/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.3536\n",
      "Epoch 19/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.2727\n",
      "Epoch 20/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.1968\n",
      "Epoch 21/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.1252\n",
      "Epoch 22/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.0641\n",
      "Epoch 23/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 1.0094\n",
      "Epoch 24/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.9579\n",
      "Epoch 25/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.9095\n",
      "Epoch 26/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.8681\n",
      "Epoch 27/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.8312\n",
      "Epoch 28/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.7942\n",
      "Epoch 29/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.7616\n",
      "Epoch 30/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 0.7347\n",
      "Epoch 31/50\n",
      "1387/1387 [==============================] - 42s 30ms/step - loss: 0.7092\n",
      "Epoch 32/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 0.6841\n",
      "Epoch 33/50\n",
      "1387/1387 [==============================] - 38s 28ms/step - loss: 0.6651\n",
      "Epoch 34/50\n",
      "1387/1387 [==============================] - 38s 27ms/step - loss: 0.6448\n",
      "Epoch 35/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.6274\n",
      "Epoch 36/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.6103\n",
      "Epoch 37/50\n",
      "1387/1387 [==============================] - 37s 27ms/step - loss: 0.5976\n",
      "Epoch 38/50\n",
      "1387/1387 [==============================] - 37s 26ms/step - loss: 0.5836\n",
      "Epoch 39/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5740\n",
      "Epoch 40/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5601\n",
      "Epoch 41/50\n",
      "1387/1387 [==============================] - 37s 26ms/step - loss: 0.5525\n",
      "Epoch 42/50\n",
      "1387/1387 [==============================] - 37s 26ms/step - loss: 0.5438\n",
      "Epoch 43/50\n",
      "1387/1387 [==============================] - 37s 26ms/step - loss: 0.5350\n",
      "Epoch 44/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5276\n",
      "Epoch 45/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5218\n",
      "Epoch 46/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5135\n",
      "Epoch 47/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5088\n",
      "Epoch 48/50\n",
      "1387/1387 [==============================] - 36s 26ms/step - loss: 0.5035\n",
      "Epoch 49/50\n",
      "1387/1387 [==============================] - 37s 26ms/step - loss: 0.5000\n",
      "Epoch 50/50\n",
      "1387/1387 [==============================] - 37s 26ms/step - loss: 0.4949\n",
      "You: hi\n",
      "Bot: how do you should take me too bad thanks i don't you should take me too bad thanks i don't you should take me too bad thanks i don't you should\n",
      "You: how are you?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\2051617351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mbot_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Bot: {bot_response}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\2051617351.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Correct spelling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \"\"\"\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     seq = text_to_word_sequence(\n\u001b[0m\u001b[0;32m    387\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mtranslate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load your dialog dataset from 'dialog.txt'\n",
    "with open('dialogs.txt', 'r', encoding='utf-8') as file:\n",
    "    dialog_data = file.read().splitlines()\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dialog_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize and pad the sequences\n",
    "input_sequences = []\n",
    "for line in dialog_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Separate input and target sequences\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# Create and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model with a specified number of epochs\n",
    "model.fit(X, y, epochs=50)  # You can specify the number of epochs here\n",
    "\n",
    "# Save the trained model to a pkl file\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Save the tokenizer to a pkl file\n",
    "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "# Initialize a spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# Load the trained model from the pkl file\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Load the tokenizer from the pkl file\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Preprocess user input\n",
    "    user_input = user_input.lower()  # Convert to lowercase\n",
    "    user_input = spell.correction(user_input)  # Correct spelling\n",
    "    input_sequence = tokenizer.texts_to_sequences([user_input])[0]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=max_sequence_length - 1, padding='pre')\n",
    "\n",
    "    # Generate a response using the trained model\n",
    "    response_sequence = []\n",
    "    for _ in range(max_sequence_length - 1):\n",
    "        predicted_word_index = model.predict(input_sequence, verbose=0).argmax()\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        response_sequence.append(predicted_word)\n",
    "        # Update the input_sequence for the next iteration\n",
    "        input_sequence[0][-1] = predicted_word_index\n",
    "\n",
    "    response = ' '.join(response_sequence)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7143b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
